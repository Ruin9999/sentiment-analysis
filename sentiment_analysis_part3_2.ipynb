{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset from the library\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset ['train']\n",
    "validation_dataset = dataset ['validation']\n",
    "test_dataset = dataset ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 8530\n",
      "Validation dataset size: 1066\n",
      "Test dataset size: 1066\n"
     ]
    }
   ],
   "source": [
    "# check the sizes of each dataset\n",
    "train_size = len(train_dataset)\n",
    "validation_size = len(validation_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print(f\"Training dataset size: {train_size}\")\n",
    "print(f\"Validation dataset size: {validation_size}\")\n",
    "print(f\"Test dataset size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "Test Dataset\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': 'lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .', 'label': 1}\n",
      "Validation Dataset\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': 'compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# view an example from each dataset\n",
    "print(\"Train Dataset\")\n",
    "print(train_dataset.features)\n",
    "print(train_dataset[0]) \n",
    "\n",
    "print(\"Test Dataset\")\n",
    "print(test_dataset.features)\n",
    "print(test_dataset[0]) \n",
    "\n",
    "print(\"Validation Dataset\")\n",
    "print(validation_dataset.features)\n",
    "print(validation_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Example:\n",
      "{'text': \"the rock is destined to be the 21st century's new conan and that he's going to make a splash even greater than arnold schwarzenegger jean claud van damme or steven segal .\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    # remove any other special characters but keep the general ones for potential sentiment usage\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\'\\!\\?\\.]', ' ', text)\n",
    "\n",
    "    # replace multiple spaces with one space only\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # remove leading and trailing whitespace to avoid unnecessary inconsistency\n",
    "    text = text.strip()\n",
    "\n",
    "    # convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "# apply the preprocessing function to the 'text' column of each dataset\n",
    "train_dataset = train_dataset.map(lambda x: {'text': preprocessing(x['text'])})\n",
    "validation_dataset = validation_dataset.map(lambda x: {'text': preprocessing(x['text'])})\n",
    "test_dataset = test_dataset.map(lambda x: {'text': preprocessing(x['text'])})\n",
    "\n",
    "# an example of the processed text\n",
    "print(\"Train Dataset Example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# empty list to store the resulting sentences\n",
    "tokenized_sentences = []\n",
    "\n",
    "for text in train_dataset['text']:\n",
    "    # Tokenize the text and append the tokenized sentence to the list\n",
    "    tokenized_sentences.append(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Size of vocabulary in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) The size of vocabulary formed in the training data is 16683\n"
     ]
    }
   ],
   "source": [
    "# empty set for storing unique words\n",
    "original_vocab = set()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        # add each word in the sentence to the words set\n",
    "        original_vocab.add(word)\n",
    "\n",
    "print(f\"(a) The size of vocabulary formed in the training data is {len(original_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Number of OOV in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b) Number of OOV words in the training data is 7866 when the minimum threshold for each word is 2\n"
     ]
    }
   ],
   "source": [
    "# adjust the parameters for word2vec\n",
    "vector_size = 100 # Dimensionality of the word vectors\n",
    "window = 3 # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 2 # Ignores all words with total frequency lower than this\n",
    "workers = 4 # CPU cores\n",
    "sg = 1 # 1 for skip-gram, 0 for CBOW\n",
    "epochs = 5 \n",
    "\n",
    "# train the word2vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences = tokenized_sentences, \n",
    "    vector_size = vector_size, \n",
    "    window = window, \n",
    "    min_count = min_count, \n",
    "    workers = workers,\n",
    "    epochs = epochs)\n",
    "\n",
    "# variable to store model's vocab list \n",
    "word2vec_vocab = set(word2vec_model.wv.key_to_index)\n",
    "\n",
    "# Calculate OOV words by comparing the original vocab and Word2Vec vocab\n",
    "oov_words = original_vocab - word2vec_vocab\n",
    "\n",
    "print(f\"(b) Number of OOV words in the training data is {len(oov_words)} when the minimum threshold for each word is {min_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Mitigating OOV - code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the UNK and PAD token\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "\n",
    "def replace_oov_words(tokenized_sentences,vocab):\n",
    "    # Replace all OOV words with <UNK>\n",
    "    # process each sentence in the tokenized_sentences list\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        # empty list to store the current processed sentence\n",
    "        processed_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                # if the current word is in the model's vocab, keep it as it is\n",
    "                processed_sentence.append(word)  \n",
    "            else:\n",
    "                # otherwise, replace the word with UNK\n",
    "                processed_sentence.append(UNK_TOKEN) \n",
    "\n",
    "        # update the sentence in the original tokenized_sentences list\n",
    "        tokenized_sentences[i] = processed_sentence\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (16685, 100)\n"
     ]
    }
   ],
   "source": [
    "# empty set for storing unique words\n",
    "final_vocab = set()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        # add each word in the sentence to the final_vocab set\n",
    "        final_vocab.add(word)\n",
    "\n",
    "# add 'UNK' and '<PAD>' to the vocabulary\n",
    "final_vocab.add(UNK_TOKEN)\n",
    "final_vocab.add(PAD_TOKEN)\n",
    "\n",
    "# create the dictionary that maps each word in final_vocab to a unique index\n",
    "word_to_index = {word: i for i, word in enumerate(final_vocab)}\n",
    "\n",
    "embedding_dim = word2vec_model.vector_size \n",
    "\n",
    "# initialize embedding matrix with number of vocab and embedding dimension\n",
    "embedding_matrix = np.zeros((len(word_to_index), embedding_dim))\n",
    "\n",
    "# fill the embedding matrix with the corresponding word vectors\n",
    "for word, i in word_to_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "    else:\n",
    "        # (option 1) random initialization for unknown words \n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "        # (option 2) use average vector for unknown words \n",
    "        # embedding_matrix[i] = np.mean(word2vec_model.wv.vectors, axis=0)\n",
    "\n",
    "print(f\"Shape of embedding matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word to indices \n",
    "def words_to_indices(sentence, word_to_index):\n",
    "    return [word_to_index.get(word, word_to_index[UNK_TOKEN]) for word in sentence.split()]\n",
    "\n",
    "train_X = [words_to_indices(sentence, word_to_index) for sentence in train_dataset['text']]\n",
    "train_y = train_dataset['label']\n",
    "val_X = [words_to_indices(sentence, word_to_index) for sentence in validation_dataset['text']]\n",
    "val_y = validation_dataset['label']\n",
    "test_X = [words_to_indices(sentence, word_to_index) for sentence in test_dataset['text']]\n",
    "test_y = test_dataset['label']\n",
    "\n",
    "def create_dataloader(X, y, batch_size=16, shuffle=True):\n",
    "    X_tensor = [torch.tensor(seq, dtype=torch.long) for seq in X]\n",
    "    X_padded = pad_sequence(X_tensor, batch_first=True, padding_value=word_to_index[PAD_TOKEN])\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_padded, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_dataloader = create_dataloader(train_X, train_y, shuffle=True)\n",
    "val_dataloader = create_dataloader(val_X, val_y, shuffle=False)\n",
    "test_dataloader = create_dataloader(test_X, test_y, shuffle=False)\n",
    "\n",
    "# convert embedding_matrix to tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Enhancement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1 Update the word embeddings during the training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNUpdateEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        pad_idx,\n",
    "        embedding_matrix,\n",
    "        freeze_embeddings=False,\n",
    "        aggregation_method=\"max_pooling\",\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # convert numpy embedding matrix to tensor\n",
    "        embedding_tensor = torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_tensor, padding_idx=pad_idx, freeze=freeze_embeddings\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # attention layer for attention aggregation method\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.aggregation_method = aggregation_method\n",
    "\n",
    "    def forward(self, text):\n",
    "        # embedded = self.embedding(text)\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        if self.aggregation_method == \"last_hidden\":\n",
    "            # use the last hidden state\n",
    "            sentence_repr = hidden.squeeze(0)\n",
    "        elif self.aggregation_method == \"last_output\":\n",
    "            # use the last output\n",
    "            sentence_repr = output[:, -1, :]\n",
    "        elif self.aggregation_method == \"mean_pooling\":\n",
    "            # average all outputs\n",
    "            sentence_repr = torch.mean(output, dim=1)\n",
    "        elif self.aggregation_method == \"max_pooling\":\n",
    "            # max pooling over the sequence\n",
    "            sentence_repr, _ = torch.max(output, dim=1)\n",
    "        elif self.aggregation_method == \"attention\":\n",
    "            # Attention mechanism\n",
    "            attention_weights = F.softmax(self.attention(output), dim=1)\n",
    "            sentence_repr = torch.sum(attention_weights * output, dim=1)\n",
    "        sentence_repr = self.dropout(sentence_repr)\n",
    "        return self.fc(sentence_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer shape: torch.Size([16685, 100])\n",
      "Embedding layer requires gradient: True\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = embedding_matrix.shape[1]  # match word2vec vector size\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "pad_idx = word_to_index[\"<PAD>\"] if \"<PAD>\" in word_to_index else 0\n",
    "\n",
    "model = RNNUpdateEmbedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    pad_idx,\n",
    "    embedding_matrix,\n",
    "    freeze_embeddings=False,\n",
    "    aggregation_method=\"max_pooling\",\n",
    ")\n",
    "\n",
    "# verify embedding layer\n",
    "print(f\"Embedding layer shape: {model.embedding.weight.shape}\")\n",
    "print(f\"Embedding layer requires gradient: {model.embedding.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: 0.5835\n",
      "Final validation accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "final_val_loss, final_val_acc = train_and_evaluate(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    best_params[\"num_epochs\"],\n",
    "    best_params[\"patience\"],\n",
    ")\n",
    "\n",
    "print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"./saved_models/part_3_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 32, 'lr': 0.0001, 'num_epochs': 50, 'patience': 5, 'weight_decay': 1e-05}\n",
      "Best validation loss: 0.5514\n",
      "Best validation accuracy: 0.7203\n",
      "Final validation loss: 0.5504\n",
      "Final validation accuracy: 0.7424\n"
     ]
    }
   ],
   "source": [
    "best_params_enhanced_rnn, best_val_loss_enhanced_rnn, best_val_acc_enhanced_rnn = (\n",
    "    hyperparameter_tuning(model, train_dataloader, val_dataloader)\n",
    ")\n",
    "print(f\"Best parameters: {best_params_enhanced_rnn}\")\n",
    "print(f\"Best validation loss: {best_val_loss_enhanced_rnn:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc_enhanced_rnn:.4f}\")\n",
    "\n",
    "# train using the best parameters\n",
    "model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=best_params_enhanced_rnn[\"lr\"],\n",
    "    weight_decay=best_params_enhanced_rnn[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "final_val_loss_enhanced_rnn, final_val_acc_enhanced_rnn = train_and_evaluate(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    best_params_enhanced_rnn[\"num_epochs\"],\n",
    "    best_params_enhanced_rnn[\"patience\"],\n",
    ")\n",
    "\n",
    "print(f\"Final validation loss: {final_val_loss_enhanced_rnn:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc_enhanced_rnn:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"./saved_models/part_3_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5417\n",
      "Test Accuracy: 0.7439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/9p9lzy_s3638t1nklml8vb400000gn/T/ipykernel_3147/1144306666.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./saved_models/part_3_1.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./saved_models/part_3_1.pth\"))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, label in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2 Mitigating OOV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigating OOV using \\<UNK\\> token and fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words 7866\n",
      "Shape of embedding matrix: (8819, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "corpus = tokenized_sentences\n",
    "fasttext_model = FastText(\n",
    "    sentences=corpus, vector_size=100, window=3, min_count=2, workers=4, sg=1\n",
    ")\n",
    "\n",
    "fasttext_vocab = set(fasttext_model.wv.key_to_index)\n",
    "\n",
    "ft_oov_words = original_vocab - fasttext_vocab\n",
    "\n",
    "print(f\"Number of OOV words\", len(ft_oov_words))\n",
    "\n",
    "tokenized_sentences = replace_oov_words(tokenized_sentences, fasttext_vocab)\n",
    "\n",
    "# empty set for storing unique words\n",
    "final_vocab = set()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        # add each word in the sentence to the final_vocab set\n",
    "        final_vocab.add(word)\n",
    "\n",
    "# add 'UNK' and '<PAD>' to the vocabulary\n",
    "final_vocab.add(UNK_TOKEN)\n",
    "final_vocab.add(PAD_TOKEN)\n",
    "\n",
    "# create the dictionary that maps each word in final_vocab to a unique index\n",
    "word_to_index = {word: i for i, word in enumerate(final_vocab)}\n",
    "\n",
    "embedding_dim = fasttext_model.vector_size\n",
    "\n",
    "# initialize embedding matrix with number of vocab and embedding dimension\n",
    "embedding_matrix = np.zeros((len(word_to_index), embedding_dim))\n",
    "\n",
    "# fill the embedding matrix with the corresponding word vectors\n",
    "#since fasttext can generate vectors for OOV words, we can directly use the vectors\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "\n",
    "print(f\"Shape of embedding matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = [\n",
    "    words_to_indices(sentence, word_to_index) for sentence in train_dataset[\"text\"]\n",
    "]\n",
    "train_y = train_dataset[\"label\"]\n",
    "val_X = [\n",
    "    words_to_indices(sentence, word_to_index) for sentence in validation_dataset[\"text\"]\n",
    "]\n",
    "val_y = validation_dataset[\"label\"]\n",
    "test_X = [\n",
    "    words_to_indices(sentence, word_to_index) for sentence in test_dataset[\"text\"]\n",
    "]\n",
    "test_y = test_dataset[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = create_dataloader(train_X, train_y, shuffle=True)\n",
    "val_dataloader = create_dataloader(val_X, val_y, shuffle=False)\n",
    "test_dataloader = create_dataloader(test_X, test_y, shuffle=False)\n",
    "\n",
    "# convert embedding_matrix to tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on RNN model with OOV elimination methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer shape: torch.Size([8819, 100])\n",
      "Embedding layer requires gradient: True\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = embedding_matrix.shape[1]  # match fasttext vector size\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "pad_idx = word_to_index[\"<PAD>\"] if \"<PAD>\" in word_to_index else 0\n",
    "\n",
    "model = RNNUpdateEmbedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    pad_idx,\n",
    "    embedding_matrix,\n",
    "    freeze_embeddings=False,\n",
    "    aggregation_method=\"max_pooling\",\n",
    ")\n",
    "\n",
    "# verify embedding layer\n",
    "print(f\"Embedding layer shape: {model.embedding.weight.shape}\")\n",
    "print(f\"Embedding layer requires gradient: {model.embedding.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: 0.5823\n",
      "Final validation accuracy: 0.7110\n"
     ]
    }
   ],
   "source": [
    "model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "final_val_loss, final_val_acc = train_and_evaluate(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    best_params[\"num_epochs\"],\n",
    "    best_params[\"patience\"],\n",
    ")\n",
    "\n",
    "print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"./saved_models/part_3_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 64, 'lr': 0.001, 'num_epochs': 50, 'patience': 5, 'weight_decay': 1e-05}\n",
      "Best validation loss: 0.5507\n",
      "Best validation accuracy: 0.7545\n",
      "Final validation loss: 0.5422\n",
      "Final validation accuracy: 0.7382\n"
     ]
    }
   ],
   "source": [
    "best_params_enhanced_rnn, best_val_loss_enhanced_rnn, best_val_acc_enhanced_rnn = (\n",
    "    hyperparameter_tuning(model, train_dataloader, val_dataloader)\n",
    ")\n",
    "print(f\"Best parameters: {best_params_enhanced_rnn}\")\n",
    "print(f\"Best validation loss: {best_val_loss_enhanced_rnn:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc_enhanced_rnn:.4f}\")\n",
    "\n",
    "# train using the best parameters\n",
    "model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=best_params_enhanced_rnn[\"lr\"],\n",
    "    weight_decay=best_params_enhanced_rnn[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "final_val_loss_enhanced_rnn, final_val_acc_enhanced_rnn = train_and_evaluate(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    best_params_enhanced_rnn[\"num_epochs\"],\n",
    "    best_params_enhanced_rnn[\"patience\"],\n",
    ")\n",
    "\n",
    "print(f\"Final validation loss: {final_val_loss_enhanced_rnn:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc_enhanced_rnn:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"./saved_models/part_3_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6206\n",
      "Test Accuracy: 0.7598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/9p9lzy_s3638t1nklml8vb400000gn/T/ipykernel_3147/2451855086.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./saved_models/part_3_2.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./saved_models/part_3_2.pth\"))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, label in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
